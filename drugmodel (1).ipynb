{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a5d7afd-5085-488a-80bb-e23b9d5df33c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Windows\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Windows\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.974\n",
      "Confusion Matrix:\n",
      "[[7767    6    5    2]\n",
      " [  17 2236   17   73]\n",
      " [  10   33 1570   25]\n",
      " [  11  121   20 1419]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import string\n",
    "color = sns.color_palette()\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn import model_selection, preprocessing, metrics, naive_bayes, linear_model\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.exceptions import NotFittedError\n",
    "\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "\n",
    "# Load data\n",
    "df1 = pd.read_csv('drugs1.tsv', sep='\\t')   \n",
    "df2 = pd.read_csv('drugs2.tsv', sep='\\t') \n",
    "df = pd.concat([df1, df2], axis=0)\n",
    "\n",
    "\n",
    "dff=df\n",
    "# Data Preprocessing\n",
    "df = df.dropna(axis=0)\n",
    "df.drop(['Unnamed: 0', 'rating', 'date', 'usefulCount'], axis=1, inplace=True)\n",
    "\n",
    "df_train = df[(df['condition'] == 'Birth Control') | \n",
    "              (df['condition'] == 'Depression') | \n",
    "              (df['condition'] == 'Pain') | \n",
    "              (df['condition'] == 'Anxiety')]\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def review_to_words(raw_review):\n",
    "    review_text = BeautifulSoup(raw_review, 'html.parser').get_text()\n",
    "    letters_only = re.sub('[^a-zA-Z]', ' ', review_text)\n",
    "    words = letters_only.lower().split()\n",
    "    meaningful_words = [w for w in words if w not in stop]\n",
    "    lemmatized_words = [lemmatizer.lemmatize(w) for w in meaningful_words]\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "# Apply preprocessing\n",
    "df_train['review_clean'] = df_train['review'].apply(review_to_words)\n",
    "\n",
    "# Feature and target variables\n",
    "X_feat = df_train['review_clean']\n",
    "y = df_train['condition']\n",
    "\n",
    "# Train-Test Split\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X_feat, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "tfidf_vectorizer2 = TfidfVectorizer(max_df=0.8, ngram_range=(1, 2))\n",
    "tfidf_train_2 = tfidf_vectorizer2.fit_transform(X_train)\n",
    "tfidf_test_2 = tfidf_vectorizer2.transform(X_test)\n",
    "\n",
    "# Model Training\n",
    "pass_tf = PassiveAggressiveClassifier()\n",
    "pass_tf.fit(tfidf_train_2, y_train)\n",
    "pred = pass_tf.predict(tfidf_test_2)\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(f\"Accuracy: {score:.3f}\")\n",
    "cm = metrics.confusion_matrix(y_test, pred, labels=df_train['condition'].unique())\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Sample sentences for recommending drugs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0701165-beae-4de3-bfdc-7b9b2aa0f3b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:   0.974\n",
      "Confusion Matrix:\n",
      "[[7763    8    7    2]\n",
      " [  18 2234   19   72]\n",
      " [  10   34 1577   17]\n",
      " [  11  122   25 1413]]\n",
      "Condition: Depression\n",
      "Top 3 Suggested Drugs:\n",
      "Sertraline\n",
      "Zoloft\n",
      "Viibryd\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Model Training\n",
    "pass_tf = PassiveAggressiveClassifier()\n",
    "pass_tf.fit(tfidf_train_2, y_train)\n",
    "pred = pass_tf.predict(tfidf_test_2)\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(\"accuracy:   %0.3f\" % score)\n",
    "cm = metrics.confusion_matrix(y_test, pred, labels=df_train['condition'].unique())\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Assuming df and other required variables (e.g., tfidf_vectorizer2, mnb_tf, review_to_words) are already defined\n",
    "df = dff\n",
    "\n",
    "def top_drugs_extractor(condition):\n",
    "    df_top = df[(df['rating'] >= 9) & (df['usefulCount'] >= 100)].sort_values(by=['rating', 'usefulCount'], ascending=[False, False])\n",
    "    drug_lst = df_top[df_top['condition'] == condition]['drugName'].head(3).tolist()\n",
    "    return drug_lst if drug_lst else ['No top drugs available']  # Handling case when no drugs are found\n",
    "\n",
    "def predict_text(lst_text):\n",
    "    df_test = pd.DataFrame(lst_text, columns=['test_sent'])\n",
    "    df_test[\"test_sent\"] = df_test[\"test_sent\"].apply(review_to_words)\n",
    "    tfidf_bigram = tfidf_vectorizer2.transform(df_test[\"test_sent\"])\n",
    "    prediction = pass_tf.predict(tfidf_bigram)\n",
    "    df_test['prediction'] = prediction\n",
    "    return df_test\n",
    "\n",
    "sentences = [\n",
    "    \"I have only been on Tekturna for 9 days. The effect was immediate. I am also on a calcium channel blocker (Tiazac) and hydrochlorothiazide. I was put on Tekturna because of palpitations experienced with Diovan (ugly drug in my opinion, same company produces both however). The palpitations were pretty bad on Diovan, 24 hour monitor by EKG etc. After a few days of substituting Tekturna for Diovan, there are no more palpitations.\",\n",
    "    \"This is the third med I've tried for anxiety and mild depression. Been on it for a week and I hate it so much. I am so dizzy, I have major diarrhea and feel worse than I started. Contacting my doc in the am and changing asap.\",\n",
    "    \"I just got diagnosed with type 2. My doctor prescribed Invokana and metformin from the beginning. My sugars went down to normal by the second week. I am losing so much weight. No side effects yet. Miracle medicine for me\"\n",
    "]\n",
    "\n",
    "# Combine sentences into one string\n",
    "combined_text = ' '.join(sentences)\n",
    "\n",
    "# Predict using the combined text\n",
    "df_predictions = predict_text([combined_text])  # Pass combined_text as a list\n",
    "\n",
    "# Extract and print results\n",
    "text = combined_text\n",
    "label = df_predictions['prediction'][0]\n",
    "    \n",
    "# Map the label to the condition\n",
    "if label == \"High Blood Pressure\":\n",
    "    target = \"High Blood Pressure\"\n",
    "elif label == \"Depression\":\n",
    "    target = \"Depression\"\n",
    "elif label == \"Diabetes, Type 2\":\n",
    "    target = \"Diabetes, Type 2\"\n",
    "else:\n",
    "    target = \"Birth Control\"\n",
    "\n",
    "top_drugs = top_drugs_extractor(target)\n",
    "\n",
    "# Print results\n",
    "print(\"Condition:\", target)\n",
    "print(\"Top 3 Suggested Drugs:\")\n",
    "for drug in top_drugs:\n",
    "    print(drug)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5d02ec4-3a88-4a6b-9dac-6060beeb2f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the TF-IDF Vectorizer\n",
    "with open('tfidf_vectorizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tfidf_vectorizer2, f)\n",
    "\n",
    "# Save the MultinomialNB model\n",
    "with open('PassiveAggressiveClassifier_model.pkl', 'wb') as f:\n",
    "    pickle.dump(pass_tf, f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28d742d-fef6-4126-9313-a68822905b96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0865768-6f7d-4126-87ec-81d3bb696d70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
